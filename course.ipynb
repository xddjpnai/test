{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Установите необходимые библиотеки\n",
        "!pip install transformers datasets peft accelerate evaluate"
      ],
      "metadata": {
        "id": "0Belly1cpyb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "import evaluate\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "U7C7alusoD_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Загрузка и подготовка данных\n",
        "def preprocess_data(example):\n",
        "    \"\"\"Форматируем данные для задачи QA.\"\"\"\n",
        "    input_text = f\"question: {example['question']} context: {example['context']}\"\n",
        "    target_text = example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
        "    return {'input_text': input_text, 'target_text': target_text}\n",
        "\n",
        "# Загрузка данных\n",
        "dataset = load_dataset(\"squad\")\n",
        "train_data = dataset['train'].map(preprocess_data, remove_columns=dataset['train'].column_names)\n",
        "validation_data = dataset['validation'].map(preprocess_data, remove_columns=dataset['validation'].column_names)"
      ],
      "metadata": {
        "id": "troR_4_Tp2Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразование данных в токены\n",
        "def tokenize_data(batch, tokenizer):\n",
        "    inputs = tokenizer(batch[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "    labels = tokenizer(batch[\"target_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\", use_fast=False)\n",
        "train_data = train_data.map(lambda x: tokenize_data(x, tokenizer), batched=True)\n",
        "validation_data = validation_data.map(lambda x: tokenize_data(x, tokenizer), batched=True)"
      ],
      "metadata": {
        "id": "GQcRi2-8p50R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаляем ненужные колонки и задаем формат\n",
        "train_data = train_data.remove_columns([\"input_text\", \"target_text\"])\n",
        "validation_data = validation_data.remove_columns([\"input_text\", \"target_text\"])\n",
        "train_data.set_format(\"torch\")\n",
        "validation_data.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "pHl09O3Dp8JO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Функция для обучения и оценки модели\n",
        "def train_and_evaluate(model, training_args, train_dataset, eval_dataset, tokenizer, config_name):\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    # Оценка модели\n",
        "    metric = evaluate.load(\"squad\")\n",
        "    results = trainer.evaluate()\n",
        "    predictions = trainer.predict(eval_dataset)\n",
        "    decoded_preds = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(predictions.label_ids, skip_special_tokens=True)\n",
        "    formatted_preds = [{\"id\": str(i), \"prediction_text\": pred} for i, pred in enumerate(decoded_preds)]\n",
        "    formatted_labels = [{\"id\": str(i), \"answers\": {\"text\": [label]}} for i, label in enumerate(decoded_labels)]\n",
        "    metrics = metric.compute(predictions=formatted_preds, references=formatted_labels)\n",
        "    metrics[\"Config\"] = config_name\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "jGZemOnQp_Bk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "source": [
        "# Функция для оценки базовых моделей\n",
        "def evaluate_baseline(model, eval_dataset, tokenizer, config_name):\n",
        "    metric = evaluate.load(\"squad\")\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    # Генерация предсказаний\n",
        "    for i in range(len(eval_dataset)):\n",
        "        input_ids = eval_dataset[i][\"input_ids\"].unsqueeze(0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        # Using max_new_tokens instead of max_length\n",
        "        output_ids = model.generate(input_ids, max_new_tokens=128, num_beams=4)\n",
        "        prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        reference = tokenizer.decode(eval_dataset[i][\"labels\"], skip_special_tokens=True)\n",
        "        predictions.append({\"id\": str(i), \"prediction_text\": prediction})\n",
        "        references.append({\"id\": str(i), \"answers\": {\"text\": [reference]}})\n",
        "\n",
        "    # Вычисление метрик\n",
        "    metrics = metric.compute(predictions=predictions, references=references)\n",
        "    metrics[\"Config\"] = config_name\n",
        "    return metrics"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "61Us0Nh4shvs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Настройки для разных экспериментов\n",
        "results = []"
      ],
      "metadata": {
        "id": "57XWiG0bqCh1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Baseline: Модель без дообучения\n",
        "for model_name in [\"facebook/opt-1.3b\", \"facebook/opt-2.7b\", \"facebook/opt-6.7b\"]:\n",
        "    model_baseline = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    metrics = evaluate_baseline(model_baseline, validation_data, tokenizer, f\"Baseline ({model_name})\")\n",
        "    results.append(metrics)"
      ],
      "metadata": {
        "id": "MulfW_ihqFiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Fine-tuning: Полная настройка модели\n",
        "for model_name in [\"facebook/opt-1.3b\", \"facebook/opt-2.7b\", \"facebook/opt-6.7b\"]:\n",
        "    model_finetune = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    training_args_finetune = TrainingArguments(\n",
        "        output_dir=f\"./results_finetune_{model_name.split('/')[-1]}\",\n",
        "        per_device_train_batch_size=4,\n",
        "        num_train_epochs=3,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        save_steps=1000,\n",
        "        logging_steps=10,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "    )\n",
        "    metrics = train_and_evaluate(\n",
        "        model_finetune, training_args_finetune, train_data, validation_data, tokenizer, f\"Full Fine-tuning ({model_name})\"\n",
        "    )\n",
        "    results.append(metrics)"
      ],
      "metadata": {
        "id": "rHrSqO6cqML6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c) LoRA: Настройка с разными значениями ранга r\n",
        "for r in [4, 8, 16]:\n",
        "    for model_name in [\"facebook/opt-1.3b\", \"facebook/opt-2.7b\", \"facebook/opt-6.7b\"]:\n",
        "        model_lora = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        lora_config = LoraConfig(\n",
        "            r=r,\n",
        "            lora_alpha=32,\n",
        "            target_modules=[\"q_proj\", \"v_proj\"],\n",
        "            lora_dropout=0.1,\n",
        "            bias=\"none\"\n",
        "        )\n",
        "        lora_model = get_peft_model(model_lora, lora_config)\n",
        "        training_args_lora = TrainingArguments(\n",
        "            output_dir=f\"./results_{model_name.split('/')[-1]}_lora_r{r}\",\n",
        "            per_device_train_batch_size=8,\n",
        "            num_train_epochs=3,\n",
        "            evaluation_strategy=\"steps\",\n",
        "            save_steps=1000,\n",
        "            logging_steps=10,\n",
        "            learning_rate=2e-4,\n",
        "            fp16=True,\n",
        "        )\n",
        "        metrics = train_and_evaluate(lora_model, training_args_lora, train_data, validation_data, tokenizer, f\"{model_name} LoRA (r={r})\")\n",
        "        results.append(metrics)"
      ],
      "metadata": {
        "id": "hZz1EshnqM3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Вывод результатов\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n",
        "results_df.to_csv(\"results_comparison.csv\", index=False)"
      ],
      "metadata": {
        "id": "ywJP3jA3qQJx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}