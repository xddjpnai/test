{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0Belly1cpyb-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ubuntu/miniconda3/lib/python3.12/site-packages (4.47.0)\n",
      "Requirement already satisfied: datasets in /home/ubuntu/miniconda3/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: peft in /home/ubuntu/miniconda3/lib/python3.12/site-packages (0.14.0)\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/miniconda3/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: evaluate in /home/ubuntu/miniconda3/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/miniconda3/lib/python3.12/site-packages (4.66.5)\n",
      "Requirement already satisfied: pytest in /home/ubuntu/miniconda3/lib/python3.12/site-packages (8.3.4)\n",
      "Requirement already satisfied: flash_attn in /home/ubuntu/miniconda3/lib/python3.12/site-packages (2.7.2.post1)\n",
      "Requirement already satisfied: tiktoken in /home/ubuntu/miniconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: einops in /home/ubuntu/miniconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from transformers) (2.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from peft) (6.1.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from peft) (2.5.1)\n",
      "Requirement already satisfied: iniconfig in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pytest) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pytest) (1.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Установите необходимые библиотеки\n",
    "!pip install transformers datasets peft accelerate evaluate tqdm pytest flash_attn tiktoken einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U7C7alusoD_b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, TrainingArguments\n",
    "from transformers import Trainer, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from evaluate import load \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "troR_4_Tp2Yo"
   },
   "outputs": [],
   "source": [
    "# Initialize necessary variables\n",
    "model_names = [\n",
    "    \"google/flan-t5-small\", \n",
    "    \"google/flan-t5-base\", \n",
    "    \"google/flan-t5-large\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQcRi2-8p50R"
   },
   "outputs": [],
   "source": [
    "# 1. Data Loading and Preparation\n",
    "def preprocess_data(example):\n",
    "    \"\"\"Format data for QA task.\"\"\"\n",
    "    input_text = f\"question: {example['question']} context: {example['context']}\"\n",
    "    target_text = example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
    "    return {'input_text': input_text, 'target_text': target_text}\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "train_data = dataset['train'].select(range(len(dataset['train']) // 10)).map(preprocess_data, remove_columns=dataset['train'].column_names)\n",
    "validation_data = dataset['validation'].select(range(len(dataset['validation']) // 10)).map(preprocess_data, remove_columns=dataset['validation'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHl09O3Dp8JO"
   },
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "def tokenize_data(batch, tokenizer):\n",
    "    inputs = tokenizer(batch[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = tokenizer(batch[\"target_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_names[0])\n",
    "train_data = train_data.map(lambda x: tokenize_data(x, tokenizer), batched=True)\n",
    "validation_data = validation_data.map(lambda x: tokenize_data(x, tokenizer), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGZemOnQp_Bk"
   },
   "outputs": [],
   "source": [
    "# Remove unnecessary columns and set format\n",
    "train_data = train_data.remove_columns([\"input_text\", \"target_text\"])\n",
    "validation_data = validation_data.remove_columns([\"input_text\", \"target_text\"])\n",
    "train_data.set_format(\"torch\")\n",
    "validation_data.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61Us0Nh4shvs"
   },
   "outputs": [],
   "source": [
    "# Helper function for training and evaluation\n",
    "def compute_metrics(pred):\n",
    "    predictions = pred.predictions\n",
    "    references = pred.label_ids\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_refs = tokenizer.batch_decode(references, skip_special_tokens=True)\n",
    "    \n",
    "    # Use the SQuAD metric to calculate F1 and Exact Match\n",
    "    results = metric.compute(predictions=decoded_preds, references=[\n",
    "        {\"id\": str(i), \"answers\": {\"text\": [ref], \"answer_start\": []}}\n",
    "        for i, ref in enumerate(decoded_refs)\n",
    "    ])\n",
    "    return {\"f1\": results[\"f1\"], \"exact_match\": results[\"exact_match\"]}\n",
    "\n",
    "def train_and_evaluate(model, training_args, train_data, eval_data, tokenizer, description):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    print(f\"Metrics for {description}: {metrics}\")\n",
    "    return metrics\n",
    "\n",
    "def evaluate_model(model, eval_data, batch_size, description):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        eval_dataset=eval_data,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./results_baseline_eval\",\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            eval_strategy=\"no\",\n",
    "            fp16=False,  # Disable mixed precision to reduce incompatibility issues\n",
    "            eval_accumulation_steps=4,  # Evaluate in smaller chunks\n",
    "            logging_dir=\"./logs\",\n",
    "        ),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    try:\n",
    "        metrics = trainer.evaluate()\n",
    "        print(f\"Metrics for {description}: {metrics}\")\n",
    "        return metrics\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(f\"CUDA OOM during evaluation {description}. Skipping.\")\n",
    "            return {\"error\": \"CUDA OOM\"}\n",
    "        elif \"OutOfResources\" in str(e):\n",
    "            print(f\"Shared memory limit exceeded during evaluation {description}. Skipping.\")\n",
    "            return {\"error\": \"OutOfResources\"}\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57XWiG0bqCh1"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "metric = load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MulfW_ihqFiu"
   },
   "outputs": [],
   "source": [
    "# Part 0: Evaluate models without any fine-tuning\n",
    "for model_name in model_names:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "    try:\n",
    "        metrics = evaluate_model(model, validation_data, batch_size=1, description=f\"Baseline ({model_name})\")\n",
    "        results.append({\"model_name\": model_name, \"method\" : '-', \"rank\" : \"-\", \"metrics\": metrics})\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e) or \"OutOfResources\" in str(e):\n",
    "            print(f\"Error during evaluation {model_name}: {e}. Skipping.\")\n",
    "            results.append({\"model_name\": model_name, \"error\": str(e)})\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "rHrSqO6cqML6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Part 1: Fine-tuning\n",
    "for model_name in model_names:\n",
    "    model_finetune = AutoModelForSeq2SeqLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "    training_args_finetune = TrainingArguments(\n",
    "        output_dir=f\"./results_finetune_{model_name.split('/')[-1]}\",\n",
    "        per_device_train_batch_size=2,  # Reduced batch size\n",
    "        gradient_accumulation_steps=4,  # Simulate larger batch size\n",
    "        num_train_epochs=3,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_steps=1000,\n",
    "        logging_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "    )\n",
    "    metrics = train_and_evaluate(\n",
    "        model_finetune, training_args_finetune, train_data, validation_data, tokenizer, f\"Full Fine-tuning ({model_name})\"\n",
    "    )\n",
    "    results.append({\"model_name\": model_name, \"method\" : 'Full-parameter', \"rank\" : \"-\", \"metrics\": metrics})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "hZz1EshnqM3P",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Part 2: LoRA with different ranks\n",
    "t5_target_modules = [\"q\", \"k\", \"v\", \"o\"]  # Simplified for T5 models\n",
    "\n",
    "for model_name in model_names:\n",
    "    for r in [4, 8, 16]:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "        # LoRA configuration\n",
    "        lora_config = LoraConfig(\n",
    "            r=r,  # Low-rank adaptation rank\n",
    "            lora_alpha=32,\n",
    "            target_modules=t5_target_modules,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\"  # Corrected for T5\n",
    "        )\n",
    "        \n",
    "        # Apply LoRA adapters\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        \n",
    "        # Ensure LoRA layers are trainable\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"lora\" in name:\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        # Training Arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results_lora_{model_name.split('/')[-1]}\",\n",
    "            per_device_train_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            num_train_epochs=3,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=5e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            save_total_limit=1,\n",
    "            gradient_checkpointing=False,  # Disabled to avoid conflicts\n",
    "        )\n",
    "    \n",
    "        # Train and evaluate\n",
    "        print(f\"Starting LoRA Fine-tuning for {model_name}...\")\n",
    "        metrics = train_and_evaluate(\n",
    "            model,\n",
    "            training_args,\n",
    "            train_data,\n",
    "            validation_data,\n",
    "            tokenizer,\n",
    "            f\"LoRA Fine-tuning ({model_name})\"\n",
    "        )\n",
    "        results.append({\"model_name\": model_name, \"method\" : 'LoRA', \"rank\" : r, \"metrics\": metrics})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywJP3jA3qQJx"
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "results_df.to_csv(\"results_comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
